{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어 QnA데이터를 이용한 단순 챗봇\n",
    "\n",
    "- 해당 노트북에서는 모델 다운로드, 토큰화, 학습(Fine-Tune)하는 과정을 설명합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터\n",
    "---\n",
    "- 해당 데이터는 [AI-Hub](https://aihub.or.kr/aidata/85)에서 신청 후 다운 받을 수 있습니다.\n",
    "- 데이터 셋은 소상공인 및 공공민원 등 10개분야에 대한 대화 데이터셋입니다.\n",
    "- 데이터 셋에서 dialog.zip에 있는 A-I 파일을 이용해 데이터 셋을 구성했습니다.\n",
    "- 사용한 데이터 셋의 발화는 총 79,940번이며, 각 대화마다 3~15번의 문장으로 구성되어있습니다.\n",
    "- 데이터의 도메인 분야는 음식점, 의류, 학원, 소매점, 생활서비스, 숙박업, 관광여가오락, 부동산으로 구성되어있습니다.\n",
    "\n",
    "\n",
    "데이터 예시는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = pd.read_excel(r\"{{enter your data path}}\\D 소매점(14,949).xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SPEAKER</th>\n",
       "      <th>SENTENCE</th>\n",
       "      <th>DOMAINID</th>\n",
       "      <th>DOMAIN</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>SPEAKERID</th>\n",
       "      <th>SENTENCEID</th>\n",
       "      <th>MAIN</th>\n",
       "      <th>SUB</th>\n",
       "      <th>QA</th>\n",
       "      <th>QACNCT</th>\n",
       "      <th>MQ</th>\n",
       "      <th>SQ</th>\n",
       "      <th>UA</th>\n",
       "      <th>SA</th>\n",
       "      <th>개체명</th>\n",
       "      <th>용어사전</th>\n",
       "      <th>지식베이스</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>고객</td>\n",
       "      <td>삼겹살 1근에 얼마에요?</td>\n",
       "      <td>D</td>\n",
       "      <td>소매</td>\n",
       "      <td>정육점</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>가격 문의</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>삼겹살 1근에 얼마에요?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>삼겹살, 1근</td>\n",
       "      <td>NaN</td>\n",
       "      <td>삼겹살/부위, 1근/중량</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>점원</td>\n",
       "      <td>만원입니다</td>\n",
       "      <td>D</td>\n",
       "      <td>소매</td>\n",
       "      <td>정육점</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>가격 문의</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>만원입니다</td>\n",
       "      <td>만원</td>\n",
       "      <td>NaN</td>\n",
       "      <td>만원/금액</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>고객</td>\n",
       "      <td>넷이 먹을건데 2근이면 되나요?</td>\n",
       "      <td>D</td>\n",
       "      <td>소매</td>\n",
       "      <td>정육점</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0인분 용량 문의</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>넷이 먹을 건데 2근이면 되나요?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>넷, 2근</td>\n",
       "      <td>NaN</td>\n",
       "      <td>넷/인원, 2근/중량</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>점원</td>\n",
       "      <td>네 충분하세요</td>\n",
       "      <td>D</td>\n",
       "      <td>소매</td>\n",
       "      <td>정육점</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0인분 용량 문의</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>네 충분하세요</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>고객</td>\n",
       "      <td>그럼 2근주세요</td>\n",
       "      <td>D</td>\n",
       "      <td>소매</td>\n",
       "      <td>정육점</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>용량별 고기 주문</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>그럼 2근 주세요</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2근</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2근/중량</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14944</th>\n",
       "      <td>고객</td>\n",
       "      <td>이 떡들 다 포장 되어있는거 맞죠?</td>\n",
       "      <td>D</td>\n",
       "      <td>소매</td>\n",
       "      <td>떡집</td>\n",
       "      <td>1</td>\n",
       "      <td>947</td>\n",
       "      <td>포장 상품 구매 확인</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>이 떡들 다 포장 되어있는거 맞죠?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14945</th>\n",
       "      <td>고객</td>\n",
       "      <td>한 박스로도 포장 되는거죠?</td>\n",
       "      <td>D</td>\n",
       "      <td>소매</td>\n",
       "      <td>떡집</td>\n",
       "      <td>1</td>\n",
       "      <td>948</td>\n",
       "      <td>포장단위 문의</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>한 박스로도 포장 되는거죠?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14946</th>\n",
       "      <td>고객</td>\n",
       "      <td>박스 말고 그냥 팩 단위로 포장해주실 수 있나요?</td>\n",
       "      <td>D</td>\n",
       "      <td>소매</td>\n",
       "      <td>떡집</td>\n",
       "      <td>1</td>\n",
       "      <td>949</td>\n",
       "      <td>포장단위 문의</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>박스 말고 그냥 팩 단위로 포장해주실 수 있나요?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14947</th>\n",
       "      <td>고객</td>\n",
       "      <td>포장하는게 너무 커서 좀 적게 포장해주시면 좋을 것 같은데요?</td>\n",
       "      <td>D</td>\n",
       "      <td>소매</td>\n",
       "      <td>떡집</td>\n",
       "      <td>1</td>\n",
       "      <td>950</td>\n",
       "      <td>포장단위 문의</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>포장하는게 너무 커서 좀 적게 포장해주시면 좋을 것 같은데요?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14948</th>\n",
       "      <td>고객</td>\n",
       "      <td>모든 떡이 다 상자 포장이 가능한거죠?</td>\n",
       "      <td>D</td>\n",
       "      <td>소매</td>\n",
       "      <td>떡집</td>\n",
       "      <td>1</td>\n",
       "      <td>951</td>\n",
       "      <td>포장단위 문의</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>모든 떡이 다 상자 포장이 가능한거죠?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14949 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SPEAKER                            SENTENCE DOMAINID DOMAIN CATEGORY  \\\n",
       "0          고객                       삼겹살 1근에 얼마에요?        D     소매      정육점   \n",
       "1          점원                               만원입니다        D     소매      정육점   \n",
       "2          고객                   넷이 먹을건데 2근이면 되나요?        D     소매      정육점   \n",
       "3          점원                             네 충분하세요        D     소매      정육점   \n",
       "4          고객                            그럼 2근주세요        D     소매      정육점   \n",
       "...       ...                                 ...      ...    ...      ...   \n",
       "14944      고객                 이 떡들 다 포장 되어있는거 맞죠?        D     소매       떡집   \n",
       "14945      고객                     한 박스로도 포장 되는거죠?        D     소매       떡집   \n",
       "14946      고객         박스 말고 그냥 팩 단위로 포장해주실 수 있나요?        D     소매       떡집   \n",
       "14947      고객  포장하는게 너무 커서 좀 적게 포장해주시면 좋을 것 같은데요?        D     소매       떡집   \n",
       "14948      고객               모든 떡이 다 상자 포장이 가능한거죠?        D     소매       떡집   \n",
       "\n",
       "       SPEAKERID  SENTENCEID         MAIN  SUB QA  QACNCT  \\\n",
       "0              1           1        가격 문의  NaN  Q     NaN   \n",
       "1              0           2        가격 문의  NaN  A     NaN   \n",
       "2              1           3    0인분 용량 문의  NaN  Q     NaN   \n",
       "3              0           4    0인분 용량 문의  NaN  A     NaN   \n",
       "4              1           5    용량별 고기 주문  NaN  Q     NaN   \n",
       "...          ...         ...          ...  ... ..     ...   \n",
       "14944          1         947  포장 상품 구매 확인  NaN  Q     NaN   \n",
       "14945          1         948      포장단위 문의  NaN  Q     NaN   \n",
       "14946          1         949      포장단위 문의  NaN  Q     NaN   \n",
       "14947          1         950      포장단위 문의  NaN  Q     NaN   \n",
       "14948          1         951      포장단위 문의  NaN  Q     NaN   \n",
       "\n",
       "                                       MQ   SQ   UA       SA      개체명 용어사전  \\\n",
       "0                           삼겹살 1근에 얼마에요?  NaN  NaN      NaN  삼겹살, 1근  NaN   \n",
       "1                                     NaN  NaN  NaN    만원입니다       만원  NaN   \n",
       "2                      넷이 먹을 건데 2근이면 되나요?  NaN  NaN      NaN    넷, 2근  NaN   \n",
       "3                                     NaN  NaN  NaN  네 충분하세요      NaN  NaN   \n",
       "4                               그럼 2근 주세요  NaN  NaN      NaN       2근  NaN   \n",
       "...                                   ...  ...  ...      ...      ...  ...   \n",
       "14944                 이 떡들 다 포장 되어있는거 맞죠?  NaN  NaN      NaN      NaN  NaN   \n",
       "14945                     한 박스로도 포장 되는거죠?  NaN  NaN      NaN      NaN  NaN   \n",
       "14946         박스 말고 그냥 팩 단위로 포장해주실 수 있나요?  NaN  NaN      NaN      NaN  NaN   \n",
       "14947  포장하는게 너무 커서 좀 적게 포장해주시면 좋을 것 같은데요?  NaN  NaN      NaN      NaN  NaN   \n",
       "14948               모든 떡이 다 상자 포장이 가능한거죠?  NaN  NaN      NaN      NaN  NaN   \n",
       "\n",
       "               지식베이스  \n",
       "0      삼겹살/부위, 1근/중량  \n",
       "1              만원/금액  \n",
       "2        넷/인원, 2근/중량  \n",
       "3                NaN  \n",
       "4              2근/중량  \n",
       "...              ...  \n",
       "14944            NaN  \n",
       "14945            NaN  \n",
       "14946            NaN  \n",
       "14947            NaN  \n",
       "14948            NaN  \n",
       "\n",
       "[14949 rows x 18 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "- 해당 데이터는 엑셀파일로 되어있으므로 학습에 필요한 데이터 셋으로 만들기 위해 전처리를 진행합니다.\n",
    "- SENTENCE와 SPEAKERID 두 컬럼을 이용하여 전처리를 진행합니다.\n",
    "- 데이터에는 하나의 질문이 두개 이상의 데이터로 구분되어있는 경우가 있기 때문에 해당 부분을 처리해줍니다.\n",
    "- 또한, 소수의 데이터가 단순 숫자로 읽히기 때문에 해당 부분을 예외처리해줍니다.\n",
    "- 해당 노트북에서는 진행과정을 보여드리기 위해 하나의 엑셀파일만으로 진행하겠습니다.\n",
    "- 만약, 노트북을 실행시키시려면 ```# 여기``` 부분을 지우고 진행해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def preprocessing(BASE_PATH, SAVE_PATH):\n",
    "    question_lists, answer_lists = [], []\n",
    "    all_data = []\n",
    "    temp_count = 0       # 여기\n",
    "    for fn in os.listdir(BASE_PATH):\n",
    "        if temp_count > 0: # 여기\n",
    "            break          # 여기\n",
    "        temp_count = 1      # 여기\n",
    "        if os.path.splitext(fn)[-1] != '.xlsx':\n",
    "            continue\n",
    "        print(fn)\n",
    "        data = pd.read_excel(os.path.join(BASE_PATH, fn))\n",
    "        all_sentence, id_list = [], []\n",
    "        for sentence, speaker_id in tqdm(zip(data[\"SENTENCE\"], data[\"SPEAKERID\"]), total=len(data)):\n",
    "            all_sentence.append(str(sentence))\n",
    "            id_list.append(speaker_id)\n",
    "\n",
    "        questions, answers = [], []\n",
    "        i = 0\n",
    "        while i < len(all_sentence):\n",
    "            end = i + 1\n",
    "            _id = id_list[i]\n",
    "            if end >= len(all_sentence):\n",
    "                break\n",
    "            while end < len(all_sentence) and id_list[end] == _id:\n",
    "                end += 1\n",
    "            if _id == 1:\n",
    "                try:\n",
    "                    questions.append(\" \".join(all_sentence[i:end]) + \"</s>\")\n",
    "                except:\n",
    "                    print(i, end)\n",
    "                    break\n",
    "            else:\n",
    "                answers.append(\" \".join(all_sentence[i:end]) + \"</s>\")\n",
    "            i = end\n",
    "        min_length = min(len(questions), len(answers))\n",
    "        question_lists.extend(questions[:min_length])\n",
    "        answer_lists.extend(answers[:min_length])\n",
    "        for i in range(min_length):\n",
    "            all_data.append(questions[i])\n",
    "            all_data.append(answers[i])\n",
    "\n",
    "    with open(SAVE_PATH, 'wt', encoding='utf-8') as f:\n",
    "        for line in all_data:\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 음식점(15,726).xlsx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ece0ff604e049e7a8eb949f27484592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15726.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIRECTORY = \"{{Enter Your Data Directory Path}}\"\n",
    "SAVE_PATH = \"{{Enter Your Save File Path}}\"\n",
    "\n",
    "preprocessing(DATA_DIRECTORY, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'지금 배달되나요?</s>\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(SAVE_PATH, 'rt', encoding='utf-8') as f:\n",
    "    data = f.readline()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로더\n",
    "\n",
    "- 위의 data를 불러올 데이터 로더 클래스를 선언 합니다.\n",
    "- 데이터를 Q와 A를 이어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ChatBotDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer):\n",
    "        self.conversation = {}\n",
    "        with open(data_path, 'rt', encoding='utf-8') as f:\n",
    "            data = f.read().split(\"\\n\")[:-1]\n",
    "            for i in range(0, len(data), 2):\n",
    "                temp_conversation = tokenizer(data[i]+data[i+1])\n",
    "                for key in temp_conversation:\n",
    "                    if key not in self.conversation:\n",
    "                        self.conversation[key] = []\n",
    "                    self.conversation[key].append(temp_conversation[key])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conversation['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.conversation.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델에 입력으로 넣어주기 위한 데이터 세트를 구성하기 위해 ```torch.utils.data.Dataset``` 객체를 import 합니다. \n",
    "\n",
    "```Dataset```객체를 이용하여 ```ConversationDataset```을 구현합니다. \n",
    "\n",
    "해당 Dataset(ConversationDataset)은 ```data_path```, ```tokenizer```를 입력으로 받습니다.\n",
    "\n",
    "```data_path```로 부터 txt 데이터를 읽어 옵니다.\n",
    "\n",
    "전처리 과정에서 Question Answer 순서로 전처리를 했기 때문에 ```data[i] + data[i+1]```로 데이터를 가져옵니다.\n",
    "\n",
    "해당 데이터는 ```Q<eos>A<eos>``` 구조를 띄고 있습니다.\n",
    "\n",
    "\n",
    "그렇게 생성된 문장을 ```tokenizer```를 이용하여 endocing 해 줍니다.\n",
    "```python\n",
    "temp_conversation = tokenizer(data[i]+data[i+1])\n",
    "```\n",
    "\n",
    "    \n",
    "```__len__```과 ```__getitem__```을 구현하여 훈련할 모델에서 데이터를 불러 올 수 있게 설정해줍니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터\n",
    "\n",
    "- DATA_PATH : Fine-Tuning에 사용할 데이터 경로를 지정합니다. 본 노트북에선 앞서 전처리한 데이터를 이용합니다.\n",
    "- MODEL_TYPE : 허깅페이스의 KoGPT-2를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = SAVE_PATH\n",
    "MODEL_TYPE = \"taeminlee/kogpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 세트를 위한 변수를 설정해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저희는 SKT-AI에서 공개한 KoGPT2를 [허깅페이스](https://huggingface.co/taeminlee/kogpt2)에서 이용가능 하게 해주기 때문에 해당 모델을 이용하여 Fine-Tuning을 진행하겠습니다.\n",
    "\n",
    "모델을 Fine-Tuning하기 위한 Tokenizer도 같이 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChatBotDataset(data_path=DATA_PATH, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 디바이스 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터 설정\n",
    "\n",
    "- BATCH_SIZE : 데이터 로더의 배치 사이즈를 설정합니다.\n",
    "- EPOCHS : Fine-Tuning할 Epoch을 설정합니다.\n",
    "- LEARNING_RATE : 모델 학습시, lr을 설정합니다.\n",
    "- WARMUP_STEPS : 스케쥴러의 warmup을 진행할 step을 설정합니다.\n",
    "- OUTPUT_FOLDER : 모델 저장 경로를 지정해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 3e-5\n",
    "WARMUP_STEPS = 100\n",
    "OUTPUT_FOLDER = r\"{{enter your save model path}}\\models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치사이즈가 결정 되었으므로 ```torch.utils.data.DataLoader```를 사용하여 모델이 학습하기 위한 데이터를 load해주는 데이터 로더를 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로더\n",
    "\n",
    "- 각 문장(input_ids)들의 길이가 서로 다르기 때문에, 배치 학습을 진행하지 못합니다.\n",
    "- 이 불편함을 해결해주는게 DataLoader에서 제공하는 [collate_fn](https://pytorch.org/docs/stable/data.html)을 이용하면 됩니다.\n",
    "- 배치 크기마다 패딩을 시켜주면 됩니다.\n",
    "- 해당 토크나이저에서는 pad_token_id = 3 입니다.\n",
    "- collate_fn에 넘겨줄 함수를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_padder(batch):\n",
    "    max_length = -1\n",
    "    pad_token_id = 3\n",
    "    train_ids, attention_mask = [], []\n",
    "    for data in batch:\n",
    "        max_length = max(max_length, len(data['input_ids']))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        train_ids.append(torch.cat([batch[i][\"input_ids\"],\n",
    "                                           torch.LongTensor([pad_token_id] * (max_length - len(batch[i][\"input_ids\"])))]))\n",
    "        attention_mask.append(torch.cat([batch[i][\"attention_mask\"],\n",
    "                                           torch.LongTensor([0] * (max_length - len(batch[i][\"attention_mask\"])))]))\n",
    "    return torch.stack(train_ids, 0), torch.stack(attention_mask, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                            collate_fn=batch_padder, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[28143, 47446, 19022,     1,   104, 25390,  3250, 43675, 47774,     1]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))\n"
     ]
    }
   ],
   "source": [
    "for d in data_loader:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained MODEL LOAD\n",
    "- 허깅페이스의 KoGPT2모델을 Load합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 선언해둔 device와 함께 GPU사용이 가능하다면, ```model.to(device)```을 이용해 모델을 GPU로 보냅니다.\n",
    "\n",
    "모델을 학습하기위해 Optimizer는 AdamW를 사용하고 scheduler를 선언 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "optimizier = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizier, WARMUP_STEPS, len(data_loader) - WARMUP_STEPS, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 모델을 저장할 곳이 없을 경우 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.mkdir(OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "모델 FineTuning 코드입니다.\n",
    "\n",
    "노트북에서 보여드리기 위하여, 간단히 ```temp_count``` 변수를 설정하여 각 epoch당 5번의 배치만 간단히 학습하는 모습을 보여드리겠습니다.\n",
    "\n",
    "실제로 실행하실경우 ```#여기```로 주석처리된 라인을 지우고 실행해주세요.\n",
    "\n",
    "에폭당 변화를 보기위하여 total_loss와 total_count를 선언하고, data_loader를 이용하여 데이터를 가져옵니다.\n",
    "\n",
    "모델과 데이터를 GPU로 보낸 후 학습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def fine_tuning_runner(model, optim, data_loader, scheduler, epochs, save_path):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    print(\"=\" * 15, \"TRAIN MODEL\", \"=\" * 15)\n",
    "    temp_count = 0              # 여기\n",
    "    for epoch in range(epochs):\n",
    "        if temp_count > 0:      # 여기\n",
    "            break               # 여기 \n",
    "        print(f'EPOCH : {epoch}, started' + \"=\" * 30)\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "        with tqdm(data_loader, desc=\"Train Epoch #{}\".format(epoch)) as t:\n",
    "            for train_ids, attention_masks in t:\n",
    "                temp_count += 1        # 여기\n",
    "                if temp_count > 5:     # 여기\n",
    "                    model.save_pretrained(os.path.join(save_path, \"temp\"))  # 여기\n",
    "                    break             # 여기\n",
    "                train_ids, attention_masks = train_ids.to(device), attention_masks.to(device)\n",
    "                outputs = model(train_ids, attention_mask=attention_masks, labels=train_ids)\n",
    "                loss = outputs[0]\n",
    "\n",
    "                total_loss += loss.detach().data\n",
    "                total_count += 1\n",
    "                t.set_postfix(loss='{:.6f}'.format(total_loss / total_count))\n",
    "                optim.zero_grad()\n",
    "                scheduler.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                scheduler.step()\n",
    "\n",
    "        model.save_pretrained(os.path.join(save_path, f\"epoch_{epoch}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== TRAIN MODEL ===============\n",
      "EPOCH : 0, started==============================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e076f73282e4c818232ff54802f59b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train Epoch #0', max=7104.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fine_tuning_runner(model, optimizier, data_loader, scheduler, EPOCHS, OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-Generate\n",
    "- FINETUNE_MODEL_PATH : 학습된 모델의 경로를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNE_MODEL_PATH = OUTPUT_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QnA_Service_MODEL에 해당 가중치를 불러와 load 시켜 줍니다.\n",
    "\n",
    "\n",
    "본 노트북에서는 진행과정을 보여드리기 위해 '/temp'에 저장되었습니다. 따라서 ```\"/temp\"``` 부분을 지워주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "QnA_Service_MODEL = GPT2LMHeadModel.from_pretrained(FINETUNE_MODEL_PATH+\"/temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"3박4일 정도 놀러가고 싶다\"\n",
    "encoded_text = tokenizer.encode(example_text, add_special_tokens=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  141, 47650, 47514, 47471,  1057,  2211, 47593,  2999,  5314]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\loveg\\Anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:963: UserWarning: `max_length` is deprecated in this function, use `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "generated_sentence = QnA_Service_MODEL.generate(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  141, 47650, 47514, 47471,  1057,  2211, 47593,  2999,  5314, 47654,\n",
       "         47447,   317, 47440,     1,     0,   104,   533, 10469,   167,  3559]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(generated_sentence[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3박4일 정도 놀러가고 싶다”고 말했다. 이 때문에 일각에서는 ‘박근혜'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
